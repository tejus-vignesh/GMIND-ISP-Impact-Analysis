# GMIND SDK - DeepLearning Module

Multi-backend object detection training and benchmarking framework supporting **TorchVision**, **Ultralytics YOLO**, and **MMDetection**.

## Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [CLI Reference](#cli-reference)
- [Architecture & Adapters](#architecture--adapters)
- [Model Reference](#model-reference)
- [Training Pipeline Features](#training-pipeline-features)
- [Testing](#testing)
- [Performance Considerations](#performance-considerations)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [References](#references)

## Installation

### Quick Start (Core Dependencies)

```bash
# Clone repository
git clone https://github.com/daramolloy/GMIND-sdk.git
cd GMIND-sdk

# Install core dependencies
pip install -e .

# Or using requirements.txt
pip install -r requirements.txt
```

### With Optional Backends

Install support for specific backends as needed:

```bash
# Install with all backends
pip install -e ".[all]"

# Or specific backends
pip install -e ".[ultralytics]"      # YOLO models
pip install -e ".[eval]"             # COCO evaluation
```

### Development Setup

```bash
# Install with dev tools (testing, linting, formatting)
pip install -e ".[all,dev]"

# Run tests
pytest tests/ -v

# Format code
black DeepLearning tests
isort DeepLearning tests
```

## Quick Start

### Train a Model

```bash
# Train using COCO dataset
python -m DeepLearning.train_models \
    --data /path/to/coco \
    --model fasterrcnn_resnet50_fpn \
    --epochs 12 \
    --batch-size 4 \
    --device cuda

# With evaluation
python -m DeepLearning.train_models \
    --data /path/to/coco \
    --model yolov8m \
    --backend ultralytics \
    --epochs 24 \
    --batch-size 8 \
    --do-eval

# Resume training
python -m DeepLearning.train_models \
    --data /path/to/coco \
    --model fasterrcnn_resnet50_fpn \
    --resume ./checkpoints/checkpoint_epoch5.pth \
    --epochs 12
```

### Run Inference on a Single Video

```bash
# Quick inference on a single video (no evaluation metrics)
python -m DeepLearning.run_inference \
    --weights yolo11m \
    --video /path/to/video.mp4 \
    --save-vid \
    --conf 0.25

# Use trained model weights
python -m DeepLearning.run_inference \
    --weights checkpoints/yolo11m/best.pt \
    --video /path/to/video.mp4 \
    --save-vid

# For full evaluation with metrics, use:
python -m Evaluation.core.baseline_detector_and_tracker \
    --config DeepLearning/gmind_config.yaml
```

### Smoke Test (Synthetic Data)

```bash
# Quick validation with synthetic data (no dataset required)
python -m tests.smoke_train --model fasterrcnn_resnet50_fpn --num-classes 5
# Or via CLI entry point:
smoke-train --model fasterrcnn_resnet50_fpn --num-classes 5
```

### Python API

```python
from DeepLearning.train_models import get_model, train_one_epoch
from torch.utils.data import DataLoader

# Load model (auto-detects backend)
model = get_model('fasterrcnn_resnet50_fpn', num_classes=80)  # TorchVision
model = get_model('yolov8m', num_classes=80)                  # Ultralytics

# Explicit backend
model = get_model('fasterrcnn_resnet50_fpn', 
                  num_classes=80, 
                  backend='torchvision')

# Get all supported models
from DeepLearning.train_models import get_all_available_models
models = get_all_available_models()
```

## CLI Reference

### train_models.py

Main training script with full configuration options.

**Required Arguments:**
- `--data PATH`: Path to COCO dataset root (contains train/val and annotations)

**Model Arguments:**
- `--model NAME`: Model identifier (default: `fasterrcnn_resnet50_fpn`)
- `--backend {torchvision,ultralytics}`: Backend selection (default: auto-detect)
- `--num-classes INT`: Number of output classes (auto-detected from annotations if not provided)

**Training Arguments:**
- `--epochs INT`: Number of training epochs (default: 12)
- `--batch-size INT`: Batch size per GPU (default: 4)
- `--lr FLOAT`: Initial learning rate (default: 0.005)
- `--num-workers INT`: DataLoader workers (default: 4)

**Optimization Arguments:**
- `--use-amp`: Enable mixed precision training (AMP)
- `--device {cuda,cpu}`: Compute device (default: auto-detect)

**Checkpoint Arguments:**
- `--checkpoint-dir PATH`: Where to save checkpoints (default: ./checkpoints)
- `--resume PATH`: Path to checkpoint for resuming training
- `--backend-config PATH`: Optional backend-specific config file
- `--backend-weights PATH`: Optional backend-specific weights file

**Evaluation Arguments:**
- `--do-eval`: Run COCO evaluation after each epoch
- `--eval-only`: Run inference/evaluation without training
- `--eval-checkpoint PATH`: Model checkpoint for eval-only mode
- `--eval-output PATH`: Save detection results to JSON
- `--eval-subset INT`: Evaluate on subset of images

**Example:**

```bash
python -m DeepLearning.train_models \
    --data /path/to/coco \
    --model fasterrcnn_resnet50_fpn \
    --epochs 24 \
    --batch-size 8 \
    --lr 0.005 \
    --use-amp \
    --do-eval \
    --checkpoint-dir ./models/fasterrcnn
```

### run_inference.py

Lightweight inference script for quick testing on a single video.

**Arguments:**
- `--weights PATH`: Model weights file (.pt) or pretrained model name (e.g., `yolo11m`, `yolov8m`)
- `--video PATH`: Path to test video file
- `--output-dir PATH`: Directory to save results (default: `inference_results`)
- `--conf FLOAT`: Confidence threshold (default: 0.25)
- `--device {cuda,cpu}`: Device to use (default: `cuda`)
- `--save-vid`: Save annotated video output
- `--save-txt`: Save predictions as text files
- `--no-show`: Don't display video in real-time

**Example:**
```bash
python -m DeepLearning.run_inference \
    --weights yolo11m \
    --video /path/to/video.mp4 \
    --save-vid \
    --conf 0.25
```

**Note:** This script is for quick testing/debugging. For full evaluation with metrics, use `Evaluation.core.baseline_detector_and_tracker`.

### smoke_train.py

Quick validation script using synthetic data (no dataset required). Located in `tests/` folder.

**Arguments:**
- `--model NAME`: Model identifier (default: `fasterrcnn_resnet50_fpn`)
- `--num-classes INT`: Number of classes (default: 5)
- `--batch-size INT`: Batch size (default: 2)
- `--image-size INT`: Synthetic image size (default: 224)

**Example:**

```bash
# Via module
python -m tests.smoke_train --model yolov8m --num-classes 10

# Via CLI entry point
smoke-train --model yolov8m --num-classes 10
```

## Architecture & Adapters

The framework uses a **adapter pattern** with a unified factory:

```python
# Single entry point for all backends
from DeepLearning.adapters import get_model

model = get_model('yolov8m', num_classes=80)  # Auto-detects Ultralytics
model = get_model('fasterrcnn_resnet50_fpn')  # Auto-detects TorchVision
model = get_model('yolov8m', backend='ultralytics')  # Explicit backend
```

### Backend Detection Logic

1. **YOLO patterns** (yolov8, yolov5, yolov9, yolov10, rtdetr) â†’ Ultralytics
2. **TorchVision patterns** (fasterrcnn, maskrcnn, retinanet, ssd, fcos) â†’ TorchVision
3. **MMDetection patterns** (faster_rcnn, cascade_rcnn, detr, etc.) â†’ MMDetection
4. **Default** â†’ TorchVision

### Adapter Architecture

```
High-Level API
    |
    v
train_models.get_model()  - Single entry point, delegates to adapters
    |
    v
adapters/__init__.py:get_model()  - Factory function, backend detection
    |
    +---> torchvision_adapter (TorchVision models)
    +---> ultralytics_adapter (YOLO models)
```

### Key Design Principles

1. **Unified Interface**: Every adapter implements consistent signatures:
   - `is_available()`: Check if library is installed
   - `get_model()`: Build and return a model
   - `inference()`: Run inference (optional)

2. **Auto-Detection**: Backend is automatically detected from model name
   - `yolov8m` â†’ Ultralytics
   - `fasterrcnn_resnet50_fpn` â†’ TorchVision
   - `faster_rcnn` â†’ MMDetection
   - Can be overridden with explicit `backend` parameter

3. **Graceful Degradation**: Missing backends raise clear errors instead of failing silently

4. **Separation of Concerns**: Each adapter is independent and self-contained

### Current Backends

#### TorchVision
- **Location**: `torchvision_adapter.py`
- **Models**: Faster R-CNN, Mask R-CNN, RetinaNet, SSD, FCOS, Keypoint R-CNN
- **Status**: Production-ready, fully tested
- **Training**: Standard PyTorch training loop (works with synthetic data)
- **Installation**: Built-in with PyTorch

#### Ultralytics
- **Location**: `ultralytics_adapter.py`
- **Models**: YOLOv5, v8, v9, v10, RT-DETR (70+ variants)
- **Status**: Production-ready, fully tested
- **Training**: Uses Ultralytics' built-in training loop (requires dataset format)
- **Installation**: `pip install ultralytics` or `uv pip install ultralytics`

#### MMDetection
- **Location**: `mmdet_adapter.py`
- **Models**: Faster R-CNN, Cascade R-CNN, DETR, YOLO variants, and 50+ architectures
- **Status**: Available and working (requires config files for model loading)
- **Training**: Standard PyTorch training loop (works with synthetic data once model is loaded)
- **Installation**: 
  ```bash
  pip install openmim
  mim install mmcv  # Installs with compiled extensions
  mim install mmdet
  ```


### Adding a New Backend

To add support for a new detection framework:

1. Create new adapter module: `DeepLearning/adapters/mynewbackend_adapter.py`

2. Implement required functions:
```python
def is_available() -> bool:
    """Check if library is installed"""
    try:
        import mynewbackend
        return True
    except:
        return False

def get_model(model_name: str, num_classes: int, pretrained: bool = True, **kwargs) -> Any:
    """Build and return a model"""
    if not is_available():
        raise ValueError("mynewbackend not installed")
    
    # Implementation here
    model = mynewbackend.load_model(model_name, num_classes)
    return model
```

3. Update `DeepLearning/adapters/__init__.py`:
   - Import the adapter: `from . import mynewbackend_adapter`
   - Add backend detection in `detect_backend()`
   - Add dispatch in `get_model()`

4. Update `DeepLearning/train_models.py`:
   - Add examples in docstring

5. Update tests in `tests/test_train_infer_all_models.py`:
   - Add backend detection tests
   - Add adapter availability tests
   - Add model loading tests

## Model Reference

This section provides a complete reference of all available object detection models across all supported backends. The `train_models.py` script supports **100+ models** from three major frameworks.

### Quick Summary

| Backend | Models Available | Status | Training Support |
|---------|-----------------|--------|------------------|
| **TorchVision** | 12 models | Fully tested | Standard PyTorch loop |
| **Ultralytics** | 70+ variants | Fully tested | Built-in training loop |
| **MMDetection** | 25+ models | Available | Standard PyTorch loop |

**Total: 100+ models available for training**

### TorchVision Models

**Backend**: `torchvision` (default)  
**Status**: Fully integrated and tested  
**Installation**: Built-in with PyTorch

#### Faster R-CNN (Two-stage detector)
Best for: High accuracy, large objects
- `fasterrcnn_resnet50_fpn` (Recommended for beginners)
- `fasterrcnn_resnet50_fpn_v2` (Improved version)
- `fasterrcnn_mobilenet_v3_large_fpn` (Mobile-friendly)
- `fasterrcnn_mobilenet_v3_large_320_fpn` (Lightweight)

#### Mask R-CNN (Two-stage + segmentation)
Best for: Instance segmentation, detailed object boundaries
- `maskrcnn_resnet50_fpn`
- `maskrcnn_resnet50_fpn_v2` (Improved)

#### RetinaNet (One-stage with focal loss)
Best for: Imbalanced datasets
- `retinanet_resnet50_fpn`
- `retinanet_resnet50_fpn_v2` (Improved)

**Note**: May have issues with custom `num_classes` due to head architecture

#### SSD - Single Shot MultiBox Detector
Best for: Speed-accuracy tradeoff
- `ssd300_vgg16` (Standard)
- `ssdlite320_mobilenet_v3_large` (Lightweight)

#### Other Models
- `fcos_resnet50_fpn` (Anchor-free detector)
- `keypointrcnn_resnet50_fpn` (Keypoint detection)

### Ultralytics YOLO Models

**Backend**: `ultralytics` (or `yolo`, `yolov8`, `yolov5`, etc.)  
**Status**: Fully integrated  
**Installation**: `pip install ultralytics` or `uv pip install ultralytics`

#### YOLOv8 - Latest Generation (30 models)
**Best for**: Production use, balanced speed/accuracy  
**Task types**: Detection, Segmentation, Pose Estimation, Oriented Bounding Box

**Detection Models (5 scales)**
```
yolov8n     (nano, 3.2M params, 80.4% mAP)
yolov8s     (small, 11.2M params, 86.6% mAP)
yolov8m     (medium, 25.9M params, 88.2% mAP) RECOMMENDED
yolov8l     (large, 43.7M params, 88.6% mAP)
yolov8x     (xlarge, 68.2M params, 88.8% mAP)
```

**Segmentation Models (5 scales)**
```
yolov8n-seg, yolov8s-seg, yolov8m-seg, yolov8l-seg, yolov8x-seg
```

**Pose Estimation (5 scales)**
```
yolov8n-pose, yolov8s-pose, yolov8m-pose, yolov8l-pose, yolov8x-pose
```

**Oriented Bounding Box / Rotated Objects (5 scales)**
```
yolov8n-obb, yolov8s-obb, yolov8m-obb, yolov8l-obb, yolov8x-obb
```

#### YOLOv5 - Stable & Proven (5 models)
**Best for**: Proven performance, legacy support  
```
yolov5n, yolov5s, yolov5m, yolov5l, yolov5x
```

#### YOLOv9 - Gradient Pathways Optimization (10 models)
**Best for**: Improved architecture with better gradient flow
```
yolov9n, yolov9s, yolov9m, yolov9c, yolov9e
yolov9n-conv, yolov9s-conv, yolov9m-conv, yolov9c-conv, yolov9e-conv
```

#### YOLOv10 - Latest Version (5 models)
**Best for**: Cutting-edge performance
```
yolov10n, yolov10s, yolov10m, yolov10l, yolov10x
```

#### RT-DETR - Real-Time Detection Transformer (2 models)
**Best for**: Transformer-based architecture with real-time speed
```
rtdetr-l, rtdetr-x
```

### MMDetection Models

**Backend**: `mmdet` or `mmdetection`  
**Status**: Available (requires config files for model loading)  
**Installation**: 
```bash
# Using pip
pip install mmdet "mmcv>=2.0.0rc4,<2.2.0"

# Using mim (recommended for compiled extensions)
pip install openmim
mim install mmcv
mim install mmdet
```

**Note**: MMDetection models require configuration files (`.py` files) to load. You can either:
- Provide explicit `--backend-config` path when training
- Clone the MMDetection repository to get configs
- Use model short names (e.g., `faster_rcnn`) which will attempt to find configs automatically

MMDetection is a comprehensive detection framework with **50+ model architectures**. The following models are supported via the adapter:

#### Two-Stage Detectors
```
faster_rcnn         - Fast R-CNN with region proposals
cascade_rcnn        - Cascaded Faster R-CNN
mask_rcnn           - R-CNN with segmentation
hybrid_task_cascade - Combines detection and segmentation
libra_rcnn          - Balanced R-CNN
```

#### One-Stage Detectors
```
ssd                 - Single Shot MultiBox Detector
retinanet           - Focal loss for hard examples
fcos                - Fully Convolutional One-Stage
atss                - Adaptive Training Sample Selection
gfl                 - Generalized Focal Loss
vfnet               - VarifocalNet
yolov3, yolov4      - YOLO variants via MMDetection
yolov5, yolov6, yolov7 - Additional YOLO variants
efficientdet        - Mobile-friendly detector
cornerdet           - Detection via corner points
centernet           - Anchor-free centerness
```

#### Transformer-Based Detectors
```
detr                - Detection Transformer (base)
deformable_detr     - Deformable attention modules
dino                - DINO: DETR with improved deNoise and Optimization
conditional_detr    - Conditional spatial queries
```

#### Anchor-Free Detectors
```
foveabox            - Detection from fovea regions
reppoints           - Representation by points
```

**Note**: To use MMDetection models, you need to provide a config file path via `--backend-config` or ensure MMDetection configs are available in the standard location.


### Model Selection Guide

#### By Use Case

**ðŸƒ Speed Priority (Real-time)**
- `yolov8n` - Ultra-fast, nano model
- `yolov5n` - Proven speed
- `ssdlite320_mobilenet_v3_large` - Lightweight

**Accuracy Priority**
- `yolov8x` - Highest accuracy YOLO
- `yolov9e` - Best YOLOv9
- `faster_rcnn_resnet50_fpn` - Traditional high accuracy
- `mask_rcnn_resnet50_fpn` - With segmentation

**Balanced (Recommended)**
- `yolov8m` - Best overall choice
- `fasterrcnn_resnet50_fpn` - TorchVision standard
- `yolov9m` - Improved YOLO

**Mobile/Edge Devices**
- `yolov8n` - Nano (3.2M params)
- `yolov5n` - Small
- `fasterrcnn_mobilenet_v3_large_fpn` - Mobile backbone

**Transfer Learning**
- Any model with pretrained weights (all support pretrained=True)
- Smaller models fine-tune faster (yolov8n, yolov5s)

**Instance Segmentation**
- `maskrcnn_resnet50_fpn` - TorchVision
- `yolov8m-seg` - YOLO segmentation
- Various MMDetection segmentation models

**ðŸŒ Rotated/Oriented Objects**
- `yolov8m-obb` - YOLO Oriented Bounding Box
- MMDetection OBB models

#### By Dataset Size

| Dataset | Recommended Model | Reason |
|---------|-------------------|--------|
| < 100 images | `yolov8n`, `yolov5n` | Small model prevents overfitting |
| 100-1K images | `yolov8s`, `fasterrcnn_mobilenet_v3` | Good balance |
| 1K-10K images | `yolov8m`, `fasterrcnn_resnet50` | Standard choice |
| 10K+ images | `yolov8l`, `yolov9m` | Larger models |
| 100K+ images | `yolov8x`, `mask_rcnn_resnet50` | Full capacity |

#### By Hardware

| Hardware | Recommended | Reason |
|----------|-------------|--------|
| GPU (consumer) | `yolov8m`, `fasterrcnn_resnet50` | Good speed/accuracy |
| GPU (high-end) | `yolov8x`, `yolov9e` | Full power |
| TPU/Mobile | `yolov8n`, `fasterrcnn_mobilenet_v3` | Optimized |
| CPU only | `yolov5n`, `ssdlite320` | Lightweight |

### Detailed Model Specifications

#### YOLOv8 Specifications

| Model | Params | Size (MB) | mAPâ‚…â‚€â‚‹â‚‰â‚… | Speed (ms) |
|-------|--------|-----------|----------|------------|
| yolov8n | 3.2M | 11.6 | 37.3 | 80 |
| yolov8s | 11.2M | 45.4 | 44.9 | 128 |
| yolov8m | 25.9M | 98.9 | 50.2 | 234 |
| yolov8l | 43.7M | 165 | 52.9 | 375 |
| yolov8x | 68.2M | 257 | 53.9 | 479 |

#### YOLOv5 Specifications

| Model | Params | Size (MB) | mAPâ‚…â‚€ | Speed (ms) |
|-------|--------|-----------|--------|------------|
| yolov5n | 1.9M | 7.7 | 45.7 | 45 |
| yolov5s | 7.2M | 28.5 | 56.8 | 98 |
| yolov5m | 21.2M | 82.5 | 60.3 | 224 |
| yolov5l | 46.5M | 177 | 62.9 | 430 |
| yolov5x | 86.7M | 331 | 64.7 | 766 |

#### TorchVision Detection Models

| Model | Backbone | mAP | Params |
|-------|----------|-----|--------|
| fasterrcnn_resnet50_fpn | ResNet-50 | 37.0 | 41.8M |
| fasterrcnn_mobilenet_v3 | MobileNetv3 | 34.3 | 16.1M |
| maskrcnn_resnet50_fpn | ResNet-50 | 37.9 (box) | 44.2M |
| ssd300_vgg16 | VGG-16 | 25.1 | 36.1M |

### Advanced: Custom Model Configuration

#### Using Custom Config (MMDetection)
```bash
# Create custom config file: my_config.py
uv run python DeepLearning/train_models.py \
    --data /path/to/coco \
    --model custom_detector \
    --backend mmdet \
    --backend-config ./configs/my_config.py \
    --epochs 24
```

#### Using Custom Weights
```bash
# Start from pre-trained weights
uv run python DeepLearning/train_models.py \
    --data /path/to/coco \
    --model yolov8m \
    --backend ultralytics \
    --backend-weights ./pretrained_weights.pt \
    --epochs 50
```

#### Resuming Training
```bash
# Resume from checkpoint
uv run python DeepLearning/train_models.py \
    --data /path/to/coco \
    --model yolov8m \
    --backend ultralytics \
    --resume ./checkpoints/checkpoint_epoch50.pth \
    --epochs 100
```

#### Function Reference

**List all available models programmatically**
```python
from DeepLearning.train_models import get_all_available_models

all_models = get_all_available_models()
for backend, models in all_models.items():
    print(f"{backend}: {len(models)} models")
    print(f"  {', '.join(models[:3])}...")
```

**Build a model for inference**
```python
from DeepLearning.train_models import get_model
from DeepLearning.adapters import get_model_from_backend

# TorchVision
model = get_model('fasterrcnn_resnet50_fpn', num_classes=91)

# Ultralytics
model = get_model_from_backend('ultralytics', 'yolov8m', num_classes=80)

# MMDetection
model = get_model_from_backend('mmdet', 'faster_rcnn', num_classes=80,
                               config_file='./configs/faster_rcnn.py')
```

## Training Pipeline Features

### Model Training
- **Mixed Precision Training (AMP)**: Faster training with reduced memory
- **Gradient Accumulation**: Support for larger effective batch sizes
- **Learning Rate Scheduling**: StepLR scheduler with configurable parameters
- **Checkpoint Management**: Save/resume training from any epoch
- **Multi-GPU Support**: DataDistributed training ready

### Evaluation
- **COCO Metrics**: Full COCO evaluation (AP, AP50, AP75, etc.)
- **Per-Epoch Validation**: Evaluate model performance during training
- **Batch Detection Results**: Save raw detection outputs as JSON

### Dataset Support
- **COCO Format**: Native support for COCO dataset structure
- **Custom Dataloaders**: Easy integration of custom datasets
- **Data Augmentation**: Random horizontal flipping, normalization

## Testing

Run the comprehensive test suite:

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_train_infer_all_models.py -v

# Run with coverage
pytest tests/ --cov=DeepLearning --cov-report=html

# Run specific test
pytest tests/test_train_infer_all_models.py::TestBackendDetection::test_detect_yolo -v
```

**Test Coverage:**
- Backend detection (3 tests)
- Adapter availability (3 tests)
- Model loading (3 tests)
- Inventory verification (4 tests)
- TorchVision inference/training (8 tests)
- Backend-specific tests (24 tests, waiting for optional libs)

**Current Status:** 30 passed, 24 skipped

## Performance Considerations

### Memory Optimization
- Mixed precision training reduces memory by ~50%
- Smaller batch sizes supported with gradient accumulation
- Model checkpointing available

### Speed Optimization
- Multi-worker DataLoader (configurable workers)
- GPU acceleration with CUDA support
- Optimized tensor operations

### Best Practices
1. Use smaller models (nano/small) for experimentation
2. Enable AMP for faster training: `--use-amp`
3. Start with small batches, increase if memory allows
4. Use eval-only mode for model selection: `--eval-only --eval-checkpoint <path>`

## Troubleshooting

### Common Issues

**ImportError: No module named 'mmdet'**
- Solution: Install MMDetection: `pip install -e ".[mmdet]"`

**CUDA out of memory**
- Reduce batch size: `--batch-size 2`
- Enable gradient accumulation (internal)
- Use smaller model: `--model fasterrcnn_mobilenet_v3_large_fpn`

**Dataset not found**
- Ensure `--data` points to COCO root with `train2017`, `val2017`, `annotations` subdirs
- Example structure:
  ```
  /path/to/coco/
  â”œâ”€â”€ train2017/
  â”œâ”€â”€ val2017/
  â””â”€â”€ annotations/
      â”œâ”€â”€ instances_train2017.json
      â””â”€â”€ instances_val2017.json
  ```

**Model loading fails**
- Verify backend is installed: `python -c "import <backend>"`
- Check model name spelling
- Use `get_all_available_models()` to list valid names

**Model Not Found**
```
Error: Model 'xyz' is not available in this torchvision build
```
**Solution**: Ensure model name is correct. Use `get_all_available_models()` to list valid options.

**Out of Memory**
```
CUDA out of memory
```
**Solution**: 
- Reduce batch size: `--batch-size 8`
- Use smaller model: `yolov8n` instead of `yolov8x`
- Enable mixed precision: `--use-amp`

**Shape Mismatch in Head**
```
RuntimeError: shape '[1, -1, 5, 100, 100]' is invalid
```
**Solution**: This is a known issue with RetinaNet. Use Faster R-CNN instead.

**Custom num_classes Issues**
```
RuntimeError: Cannot set num_classes on this model
```
**Solution**: Not all models support arbitrary num_classes. Use tested models from `get_supported_models()`.

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make changes following code style (black, isort, pylint)
4. Add tests for new functionality
5. Run test suite: `pytest tests/ -v`
6. Submit pull request

### Code Style

```bash
# Format code
black DeepLearning tests
isort DeepLearning tests

# Check for issues
pylint DeepLearning/train_models.py
mypy DeepLearning/adapters/__init__.py
```

## Project Structure

```
GMIND-SDK/
â”œâ”€â”€ DeepLearning/
â”‚   â”œâ”€â”€ adapters/              # Backend-specific adapters
â”‚   â”‚   â”œâ”€â”€ __init__.py        # Unified factory
â”‚   â”‚   â”œâ”€â”€ torchvision_adapter.py
â”‚   â”‚   â”œâ”€â”€ ultralytics_adapter.py
â”‚   â”‚   â”œâ”€â”€ mmdet_adapter.py
â”‚   â”‚   â””â”€â”€ detectron2_adapter.py
â”‚   â”œâ”€â”€ train_models.py        # Main training script (CLI + API)
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_train_infer_all_models.py  # Comprehensive test suite
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ pyproject.toml             # Project metadata and configuration
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # Root project README
```

## Configuration Files

### pyproject.toml
Standard Python packaging configuration with:
- Project metadata (name, version, description)
- Dependencies (core, optional backends, dev tools)
- Tool configurations (black, isort, pylint, mypy, pytest)
- CLI entry points

### requirements.txt
Organized by category:
- Core: PyTorch, TorchVision, utilities
- Optional: Ultralytics, MMDetection, Detectron2, evaluation
- Dev: Testing and linting tools

## References

- [PyTorch Documentation](https://pytorch.org/docs/)
- [TorchVision Detection](https://pytorch.org/vision/stable/models.html#detection)
- [Ultralytics YOLOv8](https://docs.ultralytics.com/)
- [MMDetection](https://github.com/open-mmlab/mmdetection)
- [Detectron2](https://detectron2.readthedocs.io/)
- [COCO Dataset](https://cocodataset.org/)
- [TorchVision Detection Docs](https://pytorch.org/vision/main/models.html#detection)
- [Ultralytics YOLOv8 GitHub](https://github.com/ultralytics/ultralytics)
- [MMDetection GitHub](https://github.com/open-mmlab/mmdetection)
- [Detectron2 GitHub](https://github.com/facebookresearch/detectron2)

---

**Status:** Production-ready for single and multi-model training, benchmarking, and evaluation.
